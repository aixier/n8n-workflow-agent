#!/usr/bin/env python3
"""
n8n Workflow Manager
å·¥ä½œæµç”Ÿå‘½å‘¨æœŸç®¡ç†å·¥å…·

Author: AI Terminal Team
Version: 1.0.0
"""

import json
import os
import sys
import time
import requests
import argparse
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class N8nWorkflowManager:
    """n8nå·¥ä½œæµç®¡ç†å™¨"""

    def __init__(self, base_url: str = None, api_key: str = None):
        """
        åˆå§‹åŒ–å·¥ä½œæµç®¡ç†å™¨

        Args:
            base_url: n8nå®ä¾‹URL (é»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–)
            api_key: APIå¯†é’¥ (é»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–)
        """
        self.base_url = base_url or os.getenv('N8N_BASE_URL', 'http://localhost:5678')
        self.api_key = api_key or os.getenv('N8N_API_KEY', '')

        if not self.api_key:
            # å°è¯•ä».env.localæ–‡ä»¶è¯»å–
            env_file = Path(__file__).parent / '.env.local'
            if env_file.exists():
                with open(env_file, 'r') as f:
                    for line in f:
                        if line.startswith('N8N_API_KEY='):
                            self.api_key = line.split('=', 1)[1].strip()
                            break

        self.headers = {
            'Accept': 'application/json',
            'Content-Type': 'application/json'
        }

        if self.api_key:
            self.headers['Authorization'] = f'Bearer {self.api_key}'

        self.api_url = f"{self.base_url}/api/v1"

    def test_connection(self) -> bool:
        """æµ‹è¯•n8nè¿æ¥"""
        try:
            response = requests.get(
                f"{self.api_url}/workflows",
                headers=self.headers,
                timeout=5
            )
            if response.status_code == 200:
                logger.info("âœ… Successfully connected to n8n")
                return True
            else:
                logger.error(f"âŒ Connection failed: {response.status_code}")
                return False
        except Exception as e:
            logger.error(f"âŒ Connection error: {e}")
            return False

    def create_workflow(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ›å»ºæ–°å·¥ä½œæµ

        Args:
            config: å·¥ä½œæµé…ç½®

        Returns:
            åˆ›å»ºçš„å·¥ä½œæµä¿¡æ¯
        """
        try:
            # å‡†å¤‡å·¥ä½œæµæ•°æ®
            workflow_data = {
                "name": config.get("name", f"Workflow_{datetime.now().strftime('%Y%m%d_%H%M%S')}"),
                "nodes": config.get("nodes", []),
                "connections": config.get("connections", {}),
                "active": config.get("active", False),
                "settings": config.get("settings", {
                    "executionOrder": "v1",
                    "saveManualExecutions": True,
                    "callerPolicy": "workflowsFromSameOwner"
                }),
                "staticData": config.get("staticData", None),
                "tags": config.get("tags", [])
            }

            # å‘é€åˆ›å»ºè¯·æ±‚
            response = requests.post(
                f"{self.api_url}/workflows",
                headers=self.headers,
                json=workflow_data
            )

            if response.status_code in [200, 201]:
                workflow = response.json()
                logger.info(f"âœ… Workflow created successfully: {workflow.get('id')}")
                return workflow
            else:
                logger.error(f"âŒ Failed to create workflow: {response.text}")
                return {"error": response.text, "status_code": response.status_code}

        except Exception as e:
            logger.error(f"âŒ Error creating workflow: {e}")
            return {"error": str(e)}

    def deploy_workflow(self, workflow: Dict[str, Any]) -> bool:
        """
        éƒ¨ç½²å·¥ä½œæµï¼ˆæ¿€æ´»ï¼‰

        Args:
            workflow: å·¥ä½œæµå¯¹è±¡æˆ–ID

        Returns:
            æ˜¯å¦æˆåŠŸæ¿€æ´»
        """
        try:
            workflow_id = workflow if isinstance(workflow, str) else workflow.get('id')

            # æ¿€æ´»å·¥ä½œæµ
            response = requests.patch(
                f"{self.api_url}/workflows/{workflow_id}",
                headers=self.headers,
                json={"active": True}
            )

            if response.status_code == 200:
                logger.info(f"âœ… Workflow {workflow_id} deployed successfully")
                return True
            else:
                logger.error(f"âŒ Failed to deploy workflow: {response.text}")
                return False

        except Exception as e:
            logger.error(f"âŒ Error deploying workflow: {e}")
            return False

    def update_workflow(self, workflow_id: str, changes: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ›´æ–°å·¥ä½œæµ

        Args:
            workflow_id: å·¥ä½œæµID
            changes: è¦æ›´æ–°çš„å†…å®¹

        Returns:
            æ›´æ–°åçš„å·¥ä½œæµ
        """
        try:
            # è·å–ç°æœ‰å·¥ä½œæµ
            response = requests.get(
                f"{self.api_url}/workflows/{workflow_id}",
                headers=self.headers
            )

            if response.status_code != 200:
                return {"error": f"Workflow not found: {workflow_id}"}

            workflow = response.json()

            # åº”ç”¨æ›´æ”¹
            for key, value in changes.items():
                if key == "nodes" and isinstance(value, list):
                    # æ·»åŠ æˆ–æ›´æ–°èŠ‚ç‚¹
                    workflow["nodes"] = value
                elif key == "connections":
                    # æ›´æ–°è¿æ¥
                    workflow["connections"] = value
                elif key == "add_node":
                    # æ·»åŠ å•ä¸ªèŠ‚ç‚¹
                    workflow["nodes"].append(value)
                elif key == "remove_node":
                    # ç§»é™¤èŠ‚ç‚¹
                    workflow["nodes"] = [n for n in workflow["nodes"] if n["id"] != value]
                else:
                    workflow[key] = value

            # æ›´æ–°å·¥ä½œæµ
            response = requests.patch(
                f"{self.api_url}/workflows/{workflow_id}",
                headers=self.headers,
                json=workflow
            )

            if response.status_code == 200:
                logger.info(f"âœ… Workflow {workflow_id} updated successfully")
                return response.json()
            else:
                logger.error(f"âŒ Failed to update workflow: {response.text}")
                return {"error": response.text}

        except Exception as e:
            logger.error(f"âŒ Error updating workflow: {e}")
            return {"error": str(e)}

    def delete_workflow(self, workflow_id: str) -> bool:
        """
        åˆ é™¤å·¥ä½œæµ

        Args:
            workflow_id: å·¥ä½œæµID

        Returns:
            æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        try:
            response = requests.delete(
                f"{self.api_url}/workflows/{workflow_id}",
                headers=self.headers
            )

            if response.status_code in [200, 204]:
                logger.info(f"âœ… Workflow {workflow_id} deleted successfully")
                return True
            else:
                logger.error(f"âŒ Failed to delete workflow: {response.text}")
                return False

        except Exception as e:
            logger.error(f"âŒ Error deleting workflow: {e}")
            return False

    def backup_workflow(self, workflow_id: str, backup_dir: str = "./backups") -> str:
        """
        å¤‡ä»½å·¥ä½œæµ

        Args:
            workflow_id: å·¥ä½œæµID
            backup_dir: å¤‡ä»½ç›®å½•

        Returns:
            å¤‡ä»½æ–‡ä»¶è·¯å¾„
        """
        try:
            # è·å–å·¥ä½œæµ
            response = requests.get(
                f"{self.api_url}/workflows/{workflow_id}",
                headers=self.headers
            )

            if response.status_code != 200:
                logger.error(f"âŒ Failed to get workflow for backup")
                return ""

            workflow = response.json()

            # åˆ›å»ºå¤‡ä»½ç›®å½•
            Path(backup_dir).mkdir(parents=True, exist_ok=True)

            # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = f"{backup_dir}/workflow_{workflow_id}_{timestamp}.json"

            # ä¿å­˜å¤‡ä»½
            with open(backup_file, 'w', encoding='utf-8') as f:
                json.dump(workflow, f, indent=2, ensure_ascii=False)

            logger.info(f"âœ… Workflow backed up to: {backup_file}")
            return backup_file

        except Exception as e:
            logger.error(f"âŒ Error backing up workflow: {e}")
            return ""

    def restore_workflow(self, backup_file: str) -> Dict[str, Any]:
        """
        ä»å¤‡ä»½æ¢å¤å·¥ä½œæµ

        Args:
            backup_file: å¤‡ä»½æ–‡ä»¶è·¯å¾„

        Returns:
            æ¢å¤çš„å·¥ä½œæµ
        """
        try:
            # è¯»å–å¤‡ä»½æ–‡ä»¶
            with open(backup_file, 'r', encoding='utf-8') as f:
                workflow = json.load(f)

            # ç§»é™¤IDä»¥åˆ›å»ºæ–°å·¥ä½œæµ
            workflow.pop('id', None)

            # æ·»åŠ æ¢å¤æ ‡è®°
            workflow['name'] = f"{workflow.get('name', 'Workflow')}_restored_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

            # åˆ›å»ºå·¥ä½œæµ
            return self.create_workflow(workflow)

        except Exception as e:
            logger.error(f"âŒ Error restoring workflow: {e}")
            return {"error": str(e)}

    def list_workflows(self, active_only: bool = False) -> List[Dict[str, Any]]:
        """
        åˆ—å‡ºæ‰€æœ‰å·¥ä½œæµ

        Args:
            active_only: æ˜¯å¦åªåˆ—å‡ºæ¿€æ´»çš„å·¥ä½œæµ

        Returns:
            å·¥ä½œæµåˆ—è¡¨
        """
        try:
            response = requests.get(
                f"{self.api_url}/workflows",
                headers=self.headers
            )

            if response.status_code == 200:
                workflows = response.json().get('data', [])

                if active_only:
                    workflows = [w for w in workflows if w.get('active')]

                logger.info(f"Found {len(workflows)} workflows")
                return workflows
            else:
                logger.error(f"âŒ Failed to list workflows: {response.text}")
                return []

        except Exception as e:
            logger.error(f"âŒ Error listing workflows: {e}")
            return []

    def execute_workflow(self, workflow_id: str, data: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        æ‰‹åŠ¨æ‰§è¡Œå·¥ä½œæµ

        Args:
            workflow_id: å·¥ä½œæµID
            data: è¾“å…¥æ•°æ®

        Returns:
            æ‰§è¡Œç»“æœ
        """
        try:
            execution_data = {
                "workflowData": {
                    "id": workflow_id
                }
            }

            if data:
                execution_data["data"] = data

            response = requests.post(
                f"{self.api_url}/workflows/{workflow_id}/execute",
                headers=self.headers,
                json=execution_data
            )

            if response.status_code == 200:
                result = response.json()
                logger.info(f"âœ… Workflow executed successfully: {result.get('id')}")
                return result
            else:
                logger.error(f"âŒ Failed to execute workflow: {response.text}")
                return {"error": response.text}

        except Exception as e:
            logger.error(f"âŒ Error executing workflow: {e}")
            return {"error": str(e)}

    def get_workflow_executions(self, workflow_id: str, limit: int = 10) -> List[Dict[str, Any]]:
        """
        è·å–å·¥ä½œæµæ‰§è¡Œå†å²

        Args:
            workflow_id: å·¥ä½œæµID
            limit: è¿”å›æ•°é‡é™åˆ¶

        Returns:
            æ‰§è¡Œå†å²åˆ—è¡¨
        """
        try:
            response = requests.get(
                f"{self.api_url}/executions",
                headers=self.headers,
                params={
                    "workflowId": workflow_id,
                    "limit": limit
                }
            )

            if response.status_code == 200:
                executions = response.json().get('data', [])
                logger.info(f"Found {len(executions)} executions for workflow {workflow_id}")
                return executions
            else:
                logger.error(f"âŒ Failed to get executions: {response.text}")
                return []

        except Exception as e:
            logger.error(f"âŒ Error getting executions: {e}")
            return []

    def import_workflow(self, file_path: str, activate: bool = False) -> Dict[str, Any]:
        """
        ä»æ–‡ä»¶å¯¼å…¥å·¥ä½œæµ

        Args:
            file_path: å·¥ä½œæµJSONæ–‡ä»¶è·¯å¾„
            activate: æ˜¯å¦è‡ªåŠ¨æ¿€æ´»

        Returns:
            å¯¼å…¥çš„å·¥ä½œæµ
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                workflow_config = json.load(f)

            # è®¾ç½®æ¿€æ´»çŠ¶æ€
            workflow_config['active'] = activate

            # åˆ›å»ºå·¥ä½œæµ
            return self.create_workflow(workflow_config)

        except Exception as e:
            logger.error(f"âŒ Error importing workflow: {e}")
            return {"error": str(e)}

    def export_workflow(self, workflow_id: str, output_path: str = None) -> str:
        """
        å¯¼å‡ºå·¥ä½œæµåˆ°æ–‡ä»¶

        Args:
            workflow_id: å·¥ä½œæµID
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            å¯¼å‡ºçš„æ–‡ä»¶è·¯å¾„
        """
        try:
            # è·å–å·¥ä½œæµ
            response = requests.get(
                f"{self.api_url}/workflows/{workflow_id}",
                headers=self.headers
            )

            if response.status_code != 200:
                logger.error(f"âŒ Failed to get workflow for export")
                return ""

            workflow = response.json()

            # ç”Ÿæˆè¾“å‡ºè·¯å¾„
            if not output_path:
                output_path = f"workflow_{workflow_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(workflow, f, indent=2, ensure_ascii=False)

            logger.info(f"âœ… Workflow exported to: {output_path}")
            return output_path

        except Exception as e:
            logger.error(f"âŒ Error exporting workflow: {e}")
            return ""


def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description='n8n Workflow Manager')
    parser.add_argument('--base-url', default=os.getenv('N8N_BASE_URL', 'http://localhost:5678'),
                        help='n8n base URL')
    parser.add_argument('--api-key', default=os.getenv('N8N_API_KEY', ''),
                        help='n8n API key')

    subparsers = parser.add_subparsers(dest='command', help='Commands')

    # test command
    subparsers.add_parser('test', help='Test n8n connection')

    # list command
    list_parser = subparsers.add_parser('list', help='List workflows')
    list_parser.add_argument('--active', action='store_true', help='Only show active workflows')

    # create command
    create_parser = subparsers.add_parser('create', help='Create workflow')
    create_parser.add_argument('config', help='Workflow configuration file')
    create_parser.add_argument('--activate', action='store_true', help='Activate after creation')

    # deploy command
    deploy_parser = subparsers.add_parser('deploy', help='Deploy (activate) workflow')
    deploy_parser.add_argument('workflow_id', help='Workflow ID')

    # update command
    update_parser = subparsers.add_parser('update', help='Update workflow')
    update_parser.add_argument('workflow_id', help='Workflow ID')
    update_parser.add_argument('changes', help='Changes JSON file')

    # delete command
    delete_parser = subparsers.add_parser('delete', help='Delete workflow')
    delete_parser.add_argument('workflow_id', help='Workflow ID')

    # backup command
    backup_parser = subparsers.add_parser('backup', help='Backup workflow')
    backup_parser.add_argument('workflow_id', help='Workflow ID')
    backup_parser.add_argument('--dir', default='./backups', help='Backup directory')

    # restore command
    restore_parser = subparsers.add_parser('restore', help='Restore workflow')
    restore_parser.add_argument('backup_file', help='Backup file path')

    # execute command
    execute_parser = subparsers.add_parser('execute', help='Execute workflow')
    execute_parser.add_argument('workflow_id', help='Workflow ID')
    execute_parser.add_argument('--data', help='Input data JSON file')

    # import command
    import_parser = subparsers.add_parser('import', help='Import workflow')
    import_parser.add_argument('file', help='Workflow JSON file')
    import_parser.add_argument('--activate', action='store_true', help='Activate after import')

    # export command
    export_parser = subparsers.add_parser('export', help='Export workflow')
    export_parser.add_argument('workflow_id', help='Workflow ID')
    export_parser.add_argument('--output', help='Output file path')

    args = parser.parse_args()

    # åˆå§‹åŒ–ç®¡ç†å™¨
    manager = N8nWorkflowManager(args.base_url, args.api_key)

    # æ‰§è¡Œå‘½ä»¤
    if args.command == 'test':
        manager.test_connection()

    elif args.command == 'list':
        workflows = manager.list_workflows(args.active)
        for w in workflows:
            status = "ğŸŸ¢" if w.get('active') else "âšª"
            print(f"{status} {w.get('id')} - {w.get('name')}")

    elif args.command == 'create':
        with open(args.config, 'r') as f:
            config = json.load(f)
        workflow = manager.create_workflow(config)
        if args.activate and not workflow.get('error'):
            manager.deploy_workflow(workflow)

    elif args.command == 'deploy':
        manager.deploy_workflow(args.workflow_id)

    elif args.command == 'update':
        with open(args.changes, 'r') as f:
            changes = json.load(f)
        manager.update_workflow(args.workflow_id, changes)

    elif args.command == 'delete':
        manager.delete_workflow(args.workflow_id)

    elif args.command == 'backup':
        manager.backup_workflow(args.workflow_id, args.dir)

    elif args.command == 'restore':
        manager.restore_workflow(args.backup_file)

    elif args.command == 'execute':
        data = None
        if args.data:
            with open(args.data, 'r') as f:
                data = json.load(f)
        manager.execute_workflow(args.workflow_id, data)

    elif args.command == 'import':
        manager.import_workflow(args.file, args.activate)

    elif args.command == 'export':
        manager.export_workflow(args.workflow_id, args.output)

    else:
        parser.print_help()


if __name__ == '__main__':
    main()